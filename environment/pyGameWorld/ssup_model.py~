#the file contains the code for the ssup mdoels

import random
import math
import pdb

import numpy as np
from scipy.stats import norm as gaussian_dist


#parameters for the ssup model
eps = 0.30 #value for the greedy strategy
init_pts = 3 #initial points for which the policy is inititialized
sims = 4 #number of simulation that are being run for which the average reward is calculated
iters = 5 #maximum number of simulation after which the action needs to be performed
thresh = 0.5 #reward threshold for acting
lr = 0.1 #learning rate for policy gradient


def exploration():
    if random.random()<0.3:
        return True
    else:
        return False


def gaussian_sample_policy(policy_params):
    """
    The function returns a sample from a gaussian probability 
    for given mean and standard deviation
    """
    obj_list = [policy_params['w1'], policy_params['w2'], 1-policy_params['w1']-policy_params['w1']]
    tool_no = (obj_list.index(max(obj_list))+1)
    tool_no = str(tool_no)
    pt_x = np.random.normal(policy_params['ux'+tool_no], policy_params['sx'+tool_no])
    pt_y = np.random.normal(policy_params['uy'+tool_no], policy_params['sy'+tool_no])

    return {'obj'+tool_no: [[pt_x, pt_y]]}
    
def gaussian_func(mean, std, x):
    """
    The function returns the probaility of value in a 
    gaussian distribution with mean and standard deviation 
    """
    dist = gaussian_dist(mean, std)
    return dist.pdf(x)

def gaussian_grad_mean(mean, std, x):
    """
    The function returns the value for the gaussian gradient
    for the mean
    """
    #return prob*((std**2 - ((x-mean)**2))/std**3)
    return (x-mean)/(std**2)

def gaussian_grad_std(mean, std, x):
    """
    The function returns the value for the gaussian gradient
    for the standard deviation
    """
    
    return ((((x-mean)**2 - std**2))/std**3)

def grad_tool_weight(mean, std, x, mean3, std3):
    """
    The function returns the gradient for the weight 
    of the tool
    """
    return gaussian_func(mean,std,x) - gaussian_func(mean3,std3,x)


def find_init_dist(init_world):
    """
    The function finds the min initial distance between
    the goal and object
    Args:
    init_worls(dict): world parameters
    Return
    init_dist(float): the ditance between the values
    """
    #intial position
    init_ball = init_world['objects']['Ball']['position']
    x_init_g = (
    init_world['objects']['Goal']['points'][0][0] \
    + init_world['objects']['Goal']['points'][2][0] \
    + init_world['objects']['Goal']['points'][1][0] \
    + init_world['objects']['Goal']['points'][3][0]) \
    /4
    y_init_g = (
    init_world['objects']['Goal']['points'][0][1] \
    + init_world['objects']['Goal']['points'][1][1] \
    + init_world['objects']['Goal']['points'][2][1] \
    + init_world['objects']['Goal']['points'][3][1]) \
    /4
    init_goal = [x_init_g, y_init_g]
    init_dist = np.sqrt((x_init_g-init_ball[0])**2 + (y_init_g-init_ball[1])**2)

    return init_goal, init_dist
    
def calc_reward(path, init_dist, goal_cord):
    """
    Normalized distance between target and reward
    args:
    world_act(dict) --> positions after the actions is performed
    init_world(dict) --> initial world position
    """
    if 'Goal' not in path.keys():
        #get the minimum distance of goal from object
        dist_list = [
            np.sqrt((cord[0]-goal_cord[0])**2 + (cord[1]-goal_cord[1])**2) \
            for cord in path['Ball'][0]
        ]
        dist_goal_min = min(dist_list)
        
        return (1-(dist_goal_min/init_dist))

    else:
        pdb.set_trace()
        

def init_policy():
    """
    The function initializes the parameters for policy of the given 
    enviornment
    """
    policy_param = {
        'w1':0, 'w2':0,
        'ux1':300, 'uy1':300, 'sx1':50, 'sy1':50,
        'ux2':300, 'uy2':300, 'sx2':50, 'sy2':50,
        'ux3':300, 'uy3':300, 'sx3':50, 'sy3':50,
        
    }
    return policy_param


def action(game_obj, init_pts):
    """
    The function takes the action in the real world 
    and returns the reward is calculated for the action
    """
    print("None")


def get_dynamic_obj(world_dict):
    """
    The function get's the information about he dynamic objects
    and their y vals
    Args:
    world_dict(Dict): Containing all the values for the world objects

    Return:
    dynamic_obj(Dict): Dictionary with dynamic values
    """
    #get dynamic objects y mean and x range
    dynamic_obj = {}
    for key, value in world_dict.items():
        if value['color'] in ['red', 'blue']:
            if 'position' in value.keys():
                y_mean = value['position'][1]
                x_range = [
                    value['position'][0]-value['radius'], 
                    value['position'][0]+value['radius']
                    ]
                value.update({'y_mean':y_mean, 'x_range':x_range})
                dynamic_obj.update({key:value})

            elif 'polys' in value.keys():
                x_max = 0
                x_min = 600
                y_sum = 0
                y_cnt = 0
                for row in value['polys']:
                    for col in row:
                        if col[0] > x_max:
                            x_max = col[0]
                        if col[0] < x_min:
                            x_min = col[0]
                        y_sum = y_sum + col[1]
                        y_cnt = y_cnt + 1
                
                y_mean = int(y_sum/y_cnt)        
                value.update({'y_mean':y_mean, 'x_range':[x_min, x_max]})
                dynamic_obj.update({key:value})
            else:
                print("new structure for the dynamic object")
                pdb.set_trace()

    return dynamic_obj
    
def sample(dynamic_obj, tools, tool_points_no=1, y_dist=200, x_dist=20):
    """
    Samples the points based on the object oriented priors

    Args:
    dynamic_obj(dict) -> objects dictionary coordinates that is used to sample 
    the initial points
    tools(dict)-> Tools being used in the enviornment

    Return
    init_points(dict): initialized points
    """
    
    #sample points
    init_pts = {} 
    #sample an initial point for each tool
    for tool in tools.keys():
        for t_pts in range(tool_points_no):
            #randomly sample a dynamic object
            samp_dyn_obj = random.sample(dynamic_obj.keys(),1)[0]

            #sample y points basedo n gaussian dist
            y_init = int(np.random.normal(dynamic_obj[samp_dyn_obj]['y_mean'], y_dist,1))

            #sample x uniformly
            x_0 = dynamic_obj[samp_dyn_obj]['x_range'][0]-x_dist
            x_1 = dynamic_obj[samp_dyn_obj]['x_range'][1]+x_dist
            x_init = random.sample(range(x_0, x_1), 1)[0]

            if y_init<0:
                y_init = dynamic_obj[samp_dyn_obj]['y_mean'] \
                    + (dynamic_obj[samp_dyn_obj]['y_mean'] - y_init)
                
            if tool in init_pts.keys():
                init_pts[tool].append([x_init, y_init])
            else:
                init_pts.update({tool:[[x_init, y_init]]})

    return init_pts
        

def simulate(game_obj, init_pts, init_dist, goal_cord):
    """
    Run noisy simulation based on the points to get noisy rewards
    Args:
    game_obj(dict): dictionary containing the env components
    init_pts(dict): dictionary contaiing the ponits for which the 
    simulation needs to run
    init_dist(float):initial distance bewtween goal and obj
    goal_cord(list): corrdinates for the goal
    Return:
    init_pts(dict) --> initial points for each tool
    """
    total_reward = 0
    total_cont = 0 
    for tool in init_pts.keys():
        for pts_no in init_pts[tool]:
            position = pts_no
            noise_dict = {
                'noise_position_static': 5., 'noise_position_moving': 5.,
                'noise_collision_direction': .2, 'noise_collision_elasticity': .2, 'noise_gravity': .1,
                'noise_object_friction': .1, 'noise_object_density': .1, 'noise_object_elasticity': .1
                         }

            path_dict, success, time_taken, world_act = game_obj.runFullNoisyPath(
                tool, position, maxtime=20., returnDict=True, **noise_dict
            )
            
            reward = calc_reward(path_dict, init_dist, goal_cord)
            total_reward = total_reward + reward
            total_cont = total_cont + 1

            
    return (total_reward/total_cont), success



def get_policy_gradients(policy_params, x, y, tool):
    """
    The function calculates the gradient for a each parameter
    for the given action and tool
    """
    tool_no = tool.split('obj')[1]
    grad_w1_x = grad_tool_weight(
        policy_params['ux1'], policy_params['sx1'], x,
        policy_params['ux3'], policy_params['sx3']
    )
    grad_w1_y = grad_tool_weight(
        policy_params['uy1'], policy_params['sy1'], y,
        policy_params['uy3'], policy_params['sy3']
    )
    
    grad_w1 = (grad_w1_x + grad_w1_y)/2


    grad_w2_x = grad_tool_weight(
        policy_params['ux2'], policy_params['sx2'], x,
        policy_params['ux3'], policy_params['sx3']
    )
    grad_w2_y = grad_tool_weight(
        policy_params['uy2'], policy_params['sy2'], y,
        policy_params['uy3'], policy_params['sy3']
    )
    
    grad_w2 = (grad_w2_x + grad_w2_y)/2

    
    grad_mean_x = gaussian_grad_mean(policy_params['ux'+tool_no], policy_params['sx'+tool_no], x)
    grad_mean_y = gaussian_grad_mean(policy_params['uy'+tool_no], policy_params['sy'+tool_no], y)

    grad_std_x = gaussian_grad_std(policy_params['ux'+tool_no], policy_params['sx'+tool_no], x)
    grad_std_y = gaussian_grad_std(policy_params['uy'+tool_no], policy_params['sy'+tool_no], y)

    return {
        'w1':grad_w1, 'w2':grad_w2, 'ux'+tool_no:grad_mean_x, 'uy'+tool_no:grad_mean_y,
        'sx'+tool_no: grad_std_x, 'sy'+tool_no: grad_std_y
            }
    
def calc_prob(policy_params, tool, point):
    """
    The function calculates the policy probability for a given 
    value of points 
    """
    tool_no = tool.split('obj')[1]
    
    prob_x = gaussian_func(
        policy_params['ux'+tool_no], policy_params['sx'+tool_no], point[0])

    prob_y = gaussian_func(
        policy_params['uy'+tool_no], policy_params['sy'+tool_no], point[1])
    
    return (prob_x+prob_y)/2
    
def init_gradients():
    """
    This function creates a wrapper for the gradient values and 
    then the calculated values are addated to it
    """
    policy_grad = {
        'w1':0, 'w2':0,
        'ux1':0, 'uy1':0, 'sx1':0, 'sy1':0,
        'ux2':0, 'uy2':0, 'sx2':0, 'sy2':0,
        'ux3':0, 'uy3':0, 'sx3':0, 'sy3':0   
    }
    return policy_grad

def update_policy_params(policy_params, points, reward_all_pts):
    """
    The function updates policy parameters using policy gradients for given reward
    """
    grads = init_gradients()
    total_prob = 0 
    for tools in points.keys():
        for point in points[tools]:
            policy_gradient = get_policy_gradients(policy_params, point[0], point[1], tools)
            for key, value in policy_gradient.items():
                grads[key] = grads[key] + value
                
            total_prob = total_prob + calc_prob(policy_params, tools, point)
    
    print("the gradients are ", grads)
    print("total probability denominatior is ", total_prob)
    #update the policy parameters

    for key in grads.keys():
        policy_params[key] = policy_params[key] + (reward_all_pts*lr*grads[key])/total_prob

    
    return policy_params

def SSUP_model_run(world):
    """
    The function runs the SSUP model
    (Sample, Simulate and Update model)
    Args:
    world(dict): Contains the enviornment data values
    Return:
    model_run(Pandas df): Pandas dataframe for the model run values
    best_pos(corrdinated): best coordinate values for the given enviornment
    """
    #get the dynamic_objs
    dynamic_obj = get_dynamic_obj(world._worlddict['objects'])

    #init policy
    policy_params = init_policy()
    
    #get dist between target and object and the corrdinates for goal
    goal_cord, init_dist = find_init_dist(world._worlddict)


    #ssup algorithm start
    
    #Sample ninit points from prior π(s) for each tool
    init = sample(dynamic_obj, world._tools, 3)
    
    #Simulate actions to get noisy rewards rˆ using internal model
    rewards, success = simulate(world, init, init_dist, goal_cord)

    #Initialize policy parameters θ using policy gradient on initial points
    policy_params = update_policy_params(policy_params, init, rewards)

    #Give a total of 10 tries for the agent to get the best position
    #for total_tries in range(30):

    while True:
        acting = False
        
        if exploration():
            #sample action a from prior
            action_sample = sample(dynamic_obj, world._tools, 1)
            main_obj = random.sample(action_sample.keys(),1)[0]
            
            action_sample = {main_obj: action_sample[main_obj]}
            print("Exploring!!!!!!!")

        else:
            #sample a point from the policy
            action_sample = gaussian_sample_policy(policy_params)

                    
        rewards, success = simulate(world, init, init_dist, goal_cord)
        
        #Initialize policy parameters θ using policy gradient on initial points
        policy_params = update_policy_params(policy_params, init, rewards)
        
        print("\n The reward is \n", rewards)
        print("\n The policy params are \n", policy_params)
        print("\n the success of the simulation is \n", success)
        if success == True:
            break

    print("Found the solution")
